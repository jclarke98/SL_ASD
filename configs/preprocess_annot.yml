gt:
  # Directories and paths
  audio_direc: # path to ego4d_data/v2/data/wave
  bigAnnotPath: # path to ego4d_data/v2/annotations 
  output_direc: # path to ego4d_data/sl_ASD/groundtruth

  # Utterance segmentation parameters
  min_utterance_duration: 0      # Minimum duration (seconds) for an utterance #0.1

  # Manifest and split settings
  val_ratio: 0.1                 # Fraction of training videos for validation

  # Image processing settings
  process_images: False
  img_path: # path to ego4d_data/v2/data/video_imgs # location to video images 
  annot_path: # path to ego4d/audio-visual/active-speaker-detection/active_speaker/TalkNet_ASD/Ego4d_TalkNet_ASD/data/ego4d # location to bbox annotation

pyannote:
  # Directories and paths
  audio_direc: # path to ego4d whole clip audios 
  bigAnnotPath: # path to big ego4d annotation files (av_train.json, av_val.json ...)
  output_direc: # where to output the preprocessed annotation files, clipped audio files, and bounding box face images

  # Pyannote diarization parameters
  pyannote_pipeline: pyannote/speaker-diarization-3.1
  pyannote_device: cuda
  hf_auth_token: 

  # Utterance segmentation parameters 
  min_utterance_duration: 0.01 # Minimum duration (seconds) for an utterance #0.1

  # Matching hyperparameter (overlap threshold for matching hypothesis and ground truth) [%]
  overlap_threshold: 0.15 # 0.1

  # Manifest and split settings
  val_ratio: 0.1

  # Image processing settings
  process_images: False
  img_path: # path to ego4d full-scene images 
  annot_path: # path to ego4d/audio-visual/active-speaker-detection/active_speaker/TalkNet_ASD/Ego4d_TalkNet_ASD/data/ego4d # bbox annotation path